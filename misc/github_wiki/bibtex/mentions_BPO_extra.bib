
@InProceedings{pmlr-v96-lueckmann19a,
  title = 	 {Likelihood-free inference with emulator networks},
  author =       {Lueckmann, Jan-Matthis and Bassetto, Giacomo and Karaletsos, Theofanis and Macke, Jakob H.},
  booktitle = 	 {Proceedings of The 1st Symposium on Advances in Approximate Bayesian Inference},
  pages = 	 {32--53},
  year = 	 {2019},
  editor = 	 {Ruiz, Francisco and Zhang, Cheng and Liang, Dawen and Bui, Thang},
  volume = 	 {96},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02 Dec},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v96/lueckmann19a/lueckmann19a.pdf},
  url = 	 {https://proceedings.mlr.press/v96/lueckmann19a.html},
  abstract = 	 {Approximate Bayesian Computation (ABC) provides methods for Bayesian inference in simulation-based models which do not permit tractable likelihoods. We present a new ABC method which uses probabilistic neural emulator networks to learn synthetic likelihoods on simulated data - both ’local’ emulators which approximate the likelihood for specific observed data, as well as ’global’ ones which are applicable to a range of data. Simulations are chosen adaptively using an acquisition function which takes into account uncertainty about either the posterior distribution of interest, or the parameters of the emulator. Our approach does not rely on user-defined rejection thresholds or distance functions. We illustrate inference with emulator networks on synthetic examples and on a biophysical neuron model, and show that emulators allow accurate and efficient inference even on problems which are challenging for conventional ABC approaches.}
}

@unpublished{appukuttan:hal-03586825,
  TITLE = {{A Software Framework for Validating Neuroscience Models}},
  AUTHOR = {Appukuttan, Shailesh and Sharma, Lungsi and Garcia-Rodriguez, Pedro and Davison, Andrew},
  URL = {https://hal.archives-ouvertes.fr/hal-03586825},
  NOTE = {working paper or preprint},
  YEAR = {2022},
  MONTH = Feb,
  PDF = {https://hal.archives-ouvertes.fr/hal-03586825/file/Validation__Methods__Paper___Draft___v1__Reduced_.pdf},
  HAL_ID = {hal-03586825},
  HAL_VERSION = {v1},
}

@article{doi:10.1098/rsob.220073,
  author = {Jedlicka, Peter  and Bird, Alexander D.  and Cuntz, Hermann },
  title = {Pareto optimality, economy–effectiveness trade-offs and ion channel degeneracy: improving population modelling for single neurons},
  journal = {Open Biology},
  volume = {12},
  number = {7},
  pages = {220073},
  year = {2022},
  doi = {10.1098/rsob.220073},
  URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsob.220073},
  eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsob.220073},
  abstract = { Neurons encounter unavoidable evolutionary trade-offs between multiple tasks. They must consume as little energy as possible while effectively fulfilling their functions. Cells displaying the best performance for such multi-task trade-offs are said to be Pareto optimal, with their ion channel configurations underpinning their functionality. Ion channel degeneracy, however, implies that multiple ion channel configurations can lead to functionally similar behaviour. Therefore, instead of a single model, neuroscientists often use populations of models with distinct combinations of ionic conductances. This approach is called population (database or ensemble) modelling. It remains unclear, which ion channel parameters in the vast population of functional models are more likely to be found in the brain. Here we argue that Pareto optimality can serve as a guiding principle for addressing this issue by helping to identify the subpopulations of conductance-based models that perform best for the trade-off between economy and functionality. In this way, the high-dimensional parameter space of neuronal models might be reduced to geometrically simple low-dimensional manifolds, potentially explaining experimentally observed ion channel correlations. Conversely, Pareto inference might also help deduce neuronal functions from high-dimensional Patch-seq data. In summary, Pareto optimality is a promising framework for improving population modelling of neurons and their circuits. }
}
